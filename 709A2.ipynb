{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6de744f-c7b0-4cce-8151-8230a40bda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c04d2f-55e7-4d54-ab44-1dd73c203fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1, Part A, a\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv(\"gdp_1960_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f5395b-bddb-4e90-9392-5eef192fff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10134 entries, 0 to 10133\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   year         10109 non-null  float64\n",
      " 1   rank         10134 non-null  int64  \n",
      " 2   country      10134 non-null  object \n",
      " 3   state        10134 non-null  object \n",
      " 4   gdp          10134 non-null  float64\n",
      " 5   gdp_percent  10134 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 475.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "\n",
    "df_info = df.info()\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e392292-00dd-45c8-82fe-8e16dcf1da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>rank</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>gdp</th>\n",
       "      <th>gdp_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>1</td>\n",
       "      <td>the United States</td>\n",
       "      <td>America</td>\n",
       "      <td>5.430000e+11</td>\n",
       "      <td>0.468483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>2</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>7.323397e+10</td>\n",
       "      <td>0.063149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>3</td>\n",
       "      <td>France</td>\n",
       "      <td>Europe</td>\n",
       "      <td>6.222548e+10</td>\n",
       "      <td>0.053656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>4</td>\n",
       "      <td>China</td>\n",
       "      <td>Asia</td>\n",
       "      <td>5.971647e+10</td>\n",
       "      <td>0.051493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4.430734e+10</td>\n",
       "      <td>0.038206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  rank            country    state           gdp  gdp_percent\n",
       "0  1960.0     1  the United States  America  5.430000e+11     0.468483\n",
       "1  1960.0     2     United Kingdom   Europe  7.323397e+10     0.063149\n",
       "2  1960.0     3             France   Europe  6.222548e+10     0.053656\n",
       "3  1960.0     4              China     Asia  5.971647e+10     0.051493\n",
       "4  1960.0     5              Japan     Asia  4.430734e+10     0.038206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_head = df.head()\n",
    "df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcc4ddb-bf6a-44ee-b6e9-9cde4ee588fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      year  rank                           country    state           gdp  \\\n",
       " 7      NaN     8                             India     Asia  3.702988e+10   \n",
       " 100    NaN   101  Saint Vincent and the Grenadines  America  1.306656e+07   \n",
       " 164    NaN    62                          Cameroon   Africa  6.527776e+08   \n",
       " 227    NaN    22                           Austria   Europe  7.756110e+09   \n",
       " 245    NaN    40                           Morocco   Africa  2.379606e+09   \n",
       " 284    NaN    79                             Haiti  America  2.818968e+08   \n",
       " 341    NaN    31                              Iran     Asia  4.928628e+09   \n",
       " 401    NaN    91                              Togo   Africa  1.432558e+08   \n",
       " 464    NaN    49                             Sudan   Africa  1.611333e+09   \n",
       " 2425   NaN   126                            Belize  America  1.518000e+08   \n",
       " 2451   NaN    14                      Saudi Arabia     Asia  1.650000e+11   \n",
       " 2514   NaN    77                            Garner   Africa  4.445228e+09   \n",
       " 2580   NaN   143                          Dominica  America  7.280465e+07   \n",
       " 2643   NaN    58                           Uruguay  America  1.104834e+10   \n",
       " 2688   NaN   103                           Bahamas  America  1.426500e+09   \n",
       " 2730   NaN   145                          Dominica  America  8.210739e+07   \n",
       " 2772   NaN    36                       Philippines     Asia  3.714016e+10   \n",
       " 2832   NaN    96               Congo (Brazzaville)   Africa  2.160641e+09   \n",
       " 2895   NaN     7                            Canada  America  3.410000e+11   \n",
       " 2955   NaN    67                   C Ã´ te d'Ivoire   Africa  6.838185e+09   \n",
       " 3015   NaN   127                     Liechtenstein   Europe  5.240341e+08   \n",
       " 3078   NaN    37                          Malaysia     Asia  3.394351e+10   \n",
       " 9460   NaN   109                            Cyprus   Europe  2.272918e+10   \n",
       " 9467   NaN   116                          Botswana   Africa  1.740559e+10   \n",
       " 9508   NaN   157                        Montenegro   Europe  4.856632e+09   \n",
       " \n",
       "       gdp_percent  \n",
       " 7        0.031931  \n",
       " 100      0.000011  \n",
       " 164      0.000541  \n",
       " 227      0.005874  \n",
       " 245      0.001802  \n",
       " 284      0.000213  \n",
       " 341      0.003466  \n",
       " 401      0.000101  \n",
       " 464      0.001034  \n",
       " 2425     0.000016  \n",
       " 2451     0.015396  \n",
       " 2514     0.000416  \n",
       " 2580     0.000007  \n",
       " 2643     0.001008  \n",
       " 2688     0.000130  \n",
       " 2730     0.000007  \n",
       " 2772     0.003417  \n",
       " 2832     0.000199  \n",
       " 2895     0.030631  \n",
       " 2955     0.000615  \n",
       " 3015     0.000047  \n",
       " 3078     0.002940  \n",
       " 9460     0.000284  \n",
       " 9467     0.000217  \n",
       " 9508     0.000061  ,\n",
       " Series([], dtype: int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing years\n",
    "missing_years = df[df[\"year\"].isna()]\n",
    "\n",
    "# Check for missing GDP values for specific years\n",
    "missing_gdp_per_year = df[df[\"gdp\"].isna()].groupby(\"year\").size()\n",
    "\n",
    "# Display missing year data\n",
    "missing_years, missing_gdp_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb434f5-805d-48e0-b652-d5ce40d548ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tools in c:\\users\\huang\\anaconda3\\envs\\example\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: pytils in c:\\users\\huang\\anaconda3\\envs\\example\\lib\\site-packages (from tools) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\huang\\anaconda3\\envs\\example\\lib\\site-packages (from tools) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\huang\\anaconda3\\envs\\example\\lib\\site-packages (from tools) (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bda7bcd-0dbe-4666-9e74-402811e37a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\AppData\\Local\\Temp\\ipykernel_10148\\2206382728.py:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[\"year\"] = df[\"year\"].fillna(method=\"ffill\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gdp_1960_2020_cleaned.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort dataset by country and GDP rank to ensure logical order\n",
    "df = df.sort_values(by=[\"country\", \"rank\"])\n",
    "\n",
    "# Impute missing years using forward fill\n",
    "df[\"year\"] = df[\"year\"].fillna(method=\"ffill\")\n",
    "\n",
    "# Save the cleaned dataset with imputed years for download\n",
    "cleaned_file_path = \"gdp_1960_2020_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Provide the download link\n",
    "cleaned_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd92f74-e412-4f3a-a5e4-557d8b1af221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10134 entries, 0 to 10133\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   year         10134 non-null  float64\n",
      " 1   rank         10134 non-null  int64  \n",
      " 2   country      10134 non-null  object \n",
      " 3   state        10134 non-null  object \n",
      " 4   gdp          10134 non-null  float64\n",
      " 5   gdp_percent  10134 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 475.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "df_cleaned = pd.read_csv(\"gdp_1960_2020_cleaned.csv\")\n",
    "df_cleaned_info = df_cleaned.info()\n",
    "df_cleaned_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4dfb9fa-7892-4b9b-808f-10786ae42a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Reload the cleaned dataset\n",
    "file_path = \"gdp_1960_2020_cleaned.csv\"\n",
    "df_t = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Standardize categorical columns (convert to lowercase, remove spaces/special characters)\n",
    "df_t[\"country\"] = df_t[\"country\"].str.lower().str.replace(\" \", \"_\").str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n",
    "df_t[\"state\"] = df_t[\"state\"].str.lower().str.replace(\" \", \"_\").str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n",
    "\n",
    "# Step 2: Normalize GDP values using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df_t[\"gdp_normalized\"] = scaler.fit_transform(df_t[[\"gdp\"]])\n",
    "\n",
    "# Step 3: Log transformation of GDP to reduce skewness\n",
    "df_t[\"gdp_log\"] = np.log1p(df_t[\"gdp\"])  # log(1 + GDP) to handle zero values\n",
    "\n",
    "# Step 4: Partitioning the dataset by year (for Hadoop storage optimization)\n",
    "df_t[\"partition_path\"] = df_t[\"year\"].astype(int).astype(str).apply(lambda x: f\"/gdp_data/year={x}/\")\n",
    "\n",
    "# Save transformed dataset\n",
    "transformed_file_path = \"gdp_transformed.csv\"\n",
    "df_t.to_csv(transformed_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970c95fc-6d2f-4d46-80b3-46650883a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10134 entries, 0 to 10133\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   year            10134 non-null  float64\n",
      " 1   rank            10134 non-null  int64  \n",
      " 2   country         10134 non-null  object \n",
      " 3   state           10134 non-null  object \n",
      " 4   gdp             10134 non-null  float64\n",
      " 5   gdp_percent     10134 non-null  float64\n",
      " 6   gdp_normalized  10134 non-null  float64\n",
      " 7   gdp_log         10134 non-null  float64\n",
      " 8   partition_path  10134 non-null  object \n",
      "dtypes: float64(5), int64(1), object(3)\n",
      "memory usage: 712.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041954be-e4da-4256-a7ca-dfc518437cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>rank</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>gdp</th>\n",
       "      <th>gdp_percent</th>\n",
       "      <th>gdp_normalized</th>\n",
       "      <th>gdp_log</th>\n",
       "      <th>partition_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1966.0</td>\n",
       "      <td>53</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.400000e+09</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>21.059738</td>\n",
       "      <td>/gdp_data/year=1966/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1967.0</td>\n",
       "      <td>53</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.673333e+09</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>21.238084</td>\n",
       "      <td>/gdp_data/year=1967/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1965.0</td>\n",
       "      <td>57</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.006667e+09</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>20.729910</td>\n",
       "      <td>/gdp_data/year=1965/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1968.0</td>\n",
       "      <td>61</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.373333e+09</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>21.040507</td>\n",
       "      <td>/gdp_data/year=1968/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1969.0</td>\n",
       "      <td>62</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>asia</td>\n",
       "      <td>1.408889e+09</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>21.066067</td>\n",
       "      <td>/gdp_data/year=1969/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  rank      country state           gdp  gdp_percent  gdp_normalized  \\\n",
       "0  1966.0    53  afghanistan  asia  1.400000e+09     0.000756        0.000065   \n",
       "1  1967.0    53  afghanistan  asia  1.673333e+09     0.000847        0.000078   \n",
       "2  1965.0    57  afghanistan  asia  1.006667e+09     0.000590        0.000047   \n",
       "3  1968.0    61  afghanistan  asia  1.373333e+09     0.000642        0.000064   \n",
       "4  1969.0    62  afghanistan  asia  1.408889e+09     0.000597        0.000065   \n",
       "\n",
       "     gdp_log        partition_path  \n",
       "0  21.059738  /gdp_data/year=1966/  \n",
       "1  21.238084  /gdp_data/year=1967/  \n",
       "2  20.729910  /gdp_data/year=1965/  \n",
       "3  21.040507  /gdp_data/year=1968/  \n",
       "4  21.066067  /gdp_data/year=1969/  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fcb66ca-ce6d-40e5-8c34-a1a57aedc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Read input line by line\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    parts = line.split(\",\")\n",
    "\n",
    "    # Extract columns (assuming CSV structure: year, rank, country, state, gdp, gdp_percent)\n",
    "    if len(parts) >= 5:\n",
    "        year = parts[0]\n",
    "        gdp = parts[4]\n",
    "\n",
    "        # Skip header row\n",
    "        if year != \"year\":\n",
    "            try:\n",
    "                gdp_value = float(gdp)\n",
    "                print(f\"{year}\\t{gdp_value}\")\n",
    "            except ValueError:\n",
    "                pass  # Ignore invalid data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f12e8fee-d275-4dc6-9984-213bceaed350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_year = None\n",
    "total_gdp = 0\n",
    "count = 0\n",
    "\n",
    "# Read input from Mapper\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    year, gdp = line.split(\"\\t\")\n",
    "\n",
    "    try:\n",
    "        gdp_value = float(gdp)\n",
    "    except ValueError:\n",
    "        continue  # Ignore invalid data\n",
    "\n",
    "    # Aggregating values\n",
    "    if current_year == year:\n",
    "        total_gdp += gdp_value\n",
    "        count += 1\n",
    "    else:\n",
    "        if current_year:\n",
    "            # Print average GDP for previous year\n",
    "            print(f\"{current_year}\\t{total_gdp / count}\")\n",
    "\n",
    "        current_year = year\n",
    "        total_gdp = gdp_value\n",
    "        count = 1\n",
    "\n",
    "# Print last yearâs result\n",
    "if current_year:\n",
    "    print(f\"{current_year}\\t{total_gdp / count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dcc91fc-5301-4031-9f52-03c9112946e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: double (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gdp: double (nullable = true)\n",
      " |-- gdp_percent: double (nullable = true)\n",
      " |-- gdp_normalized: double (nullable = true)\n",
      " |-- gdp_log: double (nullable = true)\n",
      " |-- partition_path: string (nullable = true)\n",
      "\n",
      "+------+----+-----------+-----+-------------+-----------+--------------------+------------------+--------------------+\n",
      "|  year|rank|    country|state|          gdp|gdp_percent|      gdp_normalized|           gdp_log|      partition_path|\n",
      "+------+----+-----------+-----+-------------+-----------+--------------------+------------------+--------------------+\n",
      "|1966.0|  53|afghanistan| asia|1.399999966E9| 7.56322E-4|6.500822862904991E-5|21.059738049996195|/gdp_data/year=1966/|\n",
      "|1967.0|  53|afghanistan| asia|1.673333417E9| 8.46769E-4|7.778082506414806E-5|21.238083532579548|/gdp_data/year=1967/|\n",
      "|1965.0|  57|afghanistan| asia|1.006666637E9| 5.90135E-4|4.662815894708959...| 20.72991035118826|/gdp_data/year=1965/|\n",
      "|1968.0|  61|afghanistan| asia|1.373333366E9| 6.41863E-4|6.376212531147175E-5|  21.0405067361543|/gdp_data/year=1968/|\n",
      "|1969.0|  62|afghanistan| asia|1.408888922E9| 5.96894E-4|6.542360057603251E-5|21.066067232830626|/gdp_data/year=1969/|\n",
      "+------+----+-----------+-----+-------------+-----------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GDP_TimeSeries_Analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the GDP dataset from HDFS\n",
    "file_path = \"gdp_transformed.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the data structure\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d589eed-e79b-4513-a716-4029e0069e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+\n",
      "|year|    country|          gdp|\n",
      "+----+-----------+-------------+\n",
      "|1966|afghanistan|1.399999966E9|\n",
      "|1967|afghanistan|1.673333417E9|\n",
      "|1965|afghanistan|1.006666637E9|\n",
      "|1968|afghanistan|1.373333366E9|\n",
      "|1969|afghanistan|1.408888922E9|\n",
      "+----+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns and filter missing values\n",
    "df = df.select(\"year\", \"country\", \"gdp\").dropna()\n",
    "\n",
    "# Convert year to numerical type\n",
    "df = df.withColumn(\"year\", col(\"year\").cast(\"integer\"))\n",
    "\n",
    "# Display processed data\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028d5428-4075-4779-9d8e-c62f1d5a664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+\n",
      "|features|         gdp|          prediction|\n",
      "+--------+------------+--------------------+\n",
      "|[1960.0]| 2.8071888E7|-6.95552269852089...|\n",
      "|[1960.0]| 8.4466654E7|-6.95552269852089...|\n",
      "|[1960.0]|     9.965E7|-6.95552269852089...|\n",
      "|[1960.0]|1.31731862E8|-6.95552269852089...|\n",
      "|[1960.0]|  1.904956E8|-6.95552269852089...|\n",
      "+--------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Prepare features for Linear Regression (Year as X, GDP as Y)\n",
    "assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "df_lr = assembler.transform(df).select(\"features\", \"gdp\")\n",
    "\n",
    "# Split into training and test sets\n",
    "train_data, test_data = df_lr.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"gdp\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Predict GDP for test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"gdp\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2041a1de-3604-4d21-8ab1-027ba15d6cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted GDP for 2025: 3681668952455.84\n",
      "Predicted GDP for 2026: 4169720739852.836\n",
      "Predicted GDP for 2027: 4910761985628.58\n",
      "Predicted GDP for 2028: 5793022414222.51\n",
      "Predicted GDP for 2029: 6455938079394.158\n",
      "Predicted GDP for 2030: 7928346652945.443\n",
      "Predicted GDP for 2031: 5447342606107.812\n",
      "Predicted GDP for 2032: 5744813174133.221\n",
      "Predicted GDP for 2033: 6030303522283.834\n",
      "Predicted GDP for 2034: 6238170394793.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for ARIMA\n",
    "df_pd = df.toPandas().sort_values(\"year\")\n",
    "\n",
    "# Fit ARIMA Model\n",
    "model = ARIMA(df_pd[\"gdp\"], order=(5,1,0))  # (p, d, q)\n",
    "arima_model = model.fit()\n",
    "\n",
    "# Predict GDP for future years\n",
    "future_years = pd.DataFrame({\"year\": range(2025, 2035)})\n",
    "future_pred = arima_model.predict(start=len(df_pd), end=len(df_pd) + 9)\n",
    "\n",
    "# Show ARIMA predictions\n",
    "for year, pred in zip(future_years[\"year\"], future_pred):\n",
    "    print(f\"Predicted GDP for {year}: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3e1072c-44b5-44e1-be6f-134dba9b55e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|year|          gdp|\n",
      "+----+-------------+\n",
      "|1966|1.399999966E9|\n",
      "|1967|1.673333417E9|\n",
      "|1965|1.006666637E9|\n",
      "|1968|1.373333366E9|\n",
      "|1969|1.408888922E9|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GDP_LinearRegression\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset from HDFS\n",
    "file_path = \"hdfs://localhost:9000/user/hadoop/gdp_data/gdp_transformed.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Select relevant columns and convert to proper data types\n",
    "df = df.select(\"year\", \"gdp\").dropna()\n",
    "df = df.withColumn(\"year\", col(\"year\").cast(\"integer\"))\n",
    "df = df.withColumn(\"gdp\", col(\"gdp\").cast(\"double\"))\n",
    "\n",
    "# Display cleaned dataset\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea13c925-9c33-451e-a376-c66ece4deced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for Linear Regression (Year as input)\n",
    "assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df).select(\"features\", \"gdp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c963108c-24b9-4479-8683-5a2cf31a174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -14320610893537.738\n",
      "Coefficient: [7270946768.64925]\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"gdp\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "print(f\"Coefficient: {lr_model.coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb04aa58-1d14-40ca-9dab-d373815734ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o520.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.126 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mtransform(df_future)\n\u001b[1;32m---> 12\u001b[0m predictions\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o520.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.126 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create DataFrame for future years (2025-2035)\n",
    "future_years = [Row(year=y) for y in range(2025, 2036)]\n",
    "df_future = spark.createDataFrame(future_years)\n",
    "\n",
    "# Convert to feature vector\n",
    "df_future = assembler.transform(df_future)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(df_future)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24eb3243-80a2-4b9e-8045-6977434a1a68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\sql\\conf.py:43\u001b[0m, in \u001b[0;36mRuntimeConfig.set\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the given Spark runtime configuration property.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mset(key, value)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'."
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.executor.memory\", \"4g\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"4g\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c32bed5-702d-4997-bdcb-105810375166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.some.config.option', 'some-value'), ('spark.app.id', 'local-1739091018588'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.port', '42741'), ('spark.driver.memory', '4g'), ('spark.driver.host', '192.168.1.126'), ('spark.executor.memory', '4g'), ('spark.executor.id', 'driver'), ('spark.app.startTime', '1739091018527'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.submitTime', '1739089522427'), ('spark.app.name', 'GDP_Prediction'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create a new Spark session with memory settings BEFORE starting it\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GDP_Prediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify configurations\n",
    "print(spark.sparkContext.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b8569f-ae2e-4d7b-9a08-8df1558cac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160.0, 12.0, 32.5, 5.5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Placeholder for actual vs predicted GDP values (Replace these with real predictions from Spark ML and ARIMA)\n",
    "# Example data: (Assuming these were generated from previous models)\n",
    "actual_gdp = np.array()\n",
    "predicted_lr = np.array()\n",
    "predicted_arima = np.array()\n",
    "# Calculate MSE and MAE for Linear Regression\n",
    "mse_lr = mean_squared_error(actual_gdp, predicted_lr)\n",
    "mae_lr = mean_absolute_error(actual_gdp, predicted_lr)\n",
    "\n",
    "# Calculate MSE and MAE for ARIMA\n",
    "mse_arima = mean_squared_error(actual_gdp, predicted_arima)\n",
    "mae_arima = mean_absolute_error(actual_gdp, predicted_arima)\n",
    "\n",
    "# Compare results\n",
    "mse_lr, mae_lr, mse_arima, mae_arima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0df762-ed3f-47af-8162-01f803f434ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create GDP category labels\n",
    "df = df.withColumn(\"gdp_class\",\n",
    "                   when(col(\"gdp\") >= 1e12, \"High GDP\")\n",
    "                   .when(col(\"gdp\") >= 5e11, \"Medium GDP\")\n",
    "                   .otherwise(\"Low GDP\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e9faebd-d9cd-4f73-9814-16c5d8b1ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.port', '43151'), ('spark.app.id', 'local-1739092176936'), ('spark.some.config.option', 'some-value'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.name', 'GDP_Classification'), ('spark.driver.memory', '4g'), ('spark.driver.host', '192.168.1.126'), ('spark.executor.memory', '4g'), ('spark.executor.id', 'driver'), ('spark.app.startTime', '1739092176885'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.submitTime', '1739089522427'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Restart Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GDP_Classification\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark is running\n",
    "print(spark.sparkContext.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b21db63-554f-4532-a223-86409edef16e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o531.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o531.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3e3831e-aaff-4471-ac71-fcc09f668a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o532.fit.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\r\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:242)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert category labels to numeric\u001b[39;00m\n\u001b[0;32m      5\u001b[0m indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdp_class\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m df_rf \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mfit(df)\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[0;32m      9\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o532.fit.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1583)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\r\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\r\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\r\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:242)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert category labels to numeric\n",
    "indexer = StringIndexer(inputCol=\"gdp_class\", outputCol=\"label\")\n",
    "df_rf = indexer.fit(df).transform(df)\n",
    "\n",
    "# Prepare features\n",
    "assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "df_rf = assembler.transform(df_rf).select(\"features\", \"label\")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df_rf.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Predict classes\n",
    "predictions = rf_model.transform(test_data)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ab3dd0d-bc69-4c49-a5d9-8a03ecf08b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KNeighborsClassifier' from 'pyspark.ml.classification' (C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\classification.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train KNN model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KNeighborsClassifier' from 'pyspark.ml.classification' (C:\\Users\\huang\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\classification.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import KNeighborsClassifier\n",
    "\n",
    "# Train KNN model\n",
    "knn = KNeighborsClassifier(featuresCol=\"features\", labelCol=\"label\", k=5)\n",
    "knn_model = knn.fit(train_data)\n",
    "\n",
    "# Predict classes\n",
    "knn_predictions = knn_model.transform(test_data)\n",
    "knn_predictions.select(\"features\", \"label\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d51e7210-83fa-4d03-84e9-fe53f6dcd313",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "gdp does not exist. Available: year, features, prediction",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate Linear Regression\u001b[39;00m\n\u001b[0;32m      4\u001b[0m lr_evaluator \u001b[38;5;241m=\u001b[39m RegressionEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m lr_rmse \u001b[38;5;241m=\u001b[39m lr_evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate Random Forest\u001b[39;00m\n\u001b[0;32m      8\u001b[0m rf_evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mevaluate(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\example\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: gdp does not exist. Available: year, features, prediction"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "lr_evaluator = RegressionEvaluator(labelCol=\"gdp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse = lr_evaluator.evaluate(predictions)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = rf_evaluator.evaluate(predictions)\n",
    "\n",
    "# Evaluate KNN\n",
    "knn_accuracy = rf_evaluator.evaluate(knn_predictions)\n",
    "\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29b3a767-f4c3-4110-9bbb-b08b88577c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m df_comparison \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metrics_comparison)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Display comparison table\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[0;32m     36\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest vs KNN Comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf_comparison)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Placeholder for actual vs predicted labels (Replace these with real predictions from Spark ML)\n",
    "# Example data: (Assuming these were generated from previous models)\n",
    "actual_labels = [0, 1, 0, 2, 1, 2, 0, 1, 2, 0]  # Example actual labels (0=Low GDP, 1=Medium GDP, 2=High GDP)\n",
    "predicted_rf = [0, 1, 0, 2, 1, 2, 0, 1, 1, 0]  # Example Random Forest predictions\n",
    "predicted_knn = [0, 1, 1, 2, 1, 2, 0, 2, 2, 0]  # Example KNN predictions\n",
    "\n",
    "# Compute evaluation metrics for Random Forest\n",
    "accuracy_rf = accuracy_score(actual_labels, predicted_rf)\n",
    "precision_rf = precision_score(actual_labels, predicted_rf, average='weighted')\n",
    "recall_rf = recall_score(actual_labels, predicted_rf, average='weighted')\n",
    "f1_rf = f1_score(actual_labels, predicted_rf, average='weighted')\n",
    "\n",
    "# Compute evaluation metrics for KNN\n",
    "accuracy_knn = accuracy_score(actual_labels, predicted_knn)\n",
    "precision_knn = precision_score(actual_labels, predicted_knn, average='weighted')\n",
    "recall_knn = recall_score(actual_labels, predicted_knn, average='weighted')\n",
    "f1_knn = f1_score(actual_labels, predicted_knn, average='weighted')\n",
    "\n",
    "# Compare results\n",
    "metrics_comparison = {\n",
    "    \"Model\": [\"Random Forest\", \"KNN\"],\n",
    "    \"Accuracy\": [accuracy_rf, accuracy_knn],\n",
    "    \"Precision\": [precision_rf, precision_knn],\n",
    "    \"Recall\": [recall_rf, recall_knn],\n",
    "    \"F1 Score\": [f1_rf, f1_knn]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(metrics_comparison)\n",
    "\n",
    "# Display comparison table\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Random Forest vs KNN Comparison\", dataframe=df_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dff30b56-5be6-4f51-95ea-c7b2ca9d8565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.897143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1 Score\n",
       "0  Random Forest       0.9      0.925     0.9  0.897143\n",
       "1            KNN       0.8      0.825     0.8  0.800000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the comparison table without using ace_tools\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare Random Forest and KNN\n",
    "metrics_comparison = {\n",
    "    \"Model\": [\"Random Forest\", \"KNN\"],\n",
    "    \"Accuracy\": [accuracy_rf, accuracy_knn],\n",
    "    \"Precision\": [precision_rf, precision_knn],\n",
    "    \"Recall\": [recall_rf, recall_knn],\n",
    "    \"F1 Score\": [f1_rf, f1_knn]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(metrics_comparison)\n",
    "\n",
    "# Display the comparison table\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a bar chart to compare model performance\n",
    "df_comparison.set_index(\"Model\").plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Random Forest vs KNN: Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)  # Ensure scale is between 0 and 1 for easier comparison\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Return the comparison DataFrame\n",
    "df_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932ecb4-60e1-43d8-b813-25030131475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
